{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPO5d/aFewynb0ShlIJb95J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moon-123/BPE-WPT-Report/blob/main/BPE_Report.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 논문을 이해하는데 알아두면 좋은 개념들\n",
        "### 1-1. 통계 기반 기계번역(SMT)\n",
        "* 병렬 말뭉치를 기반으로 번역 모델을 학습.\n",
        "> 병렬 말뭉치란?  \n",
        "> 영어-프랑스어 번역 말뭉치, 영어-한국어 번역 말뭉치\n",
        "* 각 단어 또는 구의 번역 확률을 추정하여 학습함.\n",
        "* 학습된 모델은 확률을 기반으로 최적의 번역을 찾아냄.\n",
        "* ML기법이라고 생각\n",
        "\n",
        "### 1-2. 구문 기반 기계번역(PBMT)\n",
        "* 병렬 텍스트 쌍을 사용하여 학습.\n",
        "* 각 구와 번역된 구의 대응 관계가 훈련됨.\n",
        "* SMT의 기법중 하나\n",
        "\n",
        "### 1-3. 문자 기반 기계번역? -> 서브워드 유닛을 사용한 번역 기법이라고 생각하면 됨.\n",
        "* 서브워드 유닛은 인공신경망 기계 번역에만 사용되는 것은 아님.\n",
        "* 따라서 문자 기반이라고 해도 여러가지 방법이 있다...\n",
        "* 단어 기반 기계 번역과 다름\n",
        "\n",
        "### 1-4. 어텐션 메커니즘\n",
        "* 입력 시퀀스의 특정 부분에 '집중'하게 한다.\n",
        "* 현재 문장에서 번역되는 어떤  부분이 원문의 어떤 단어에 주로 의존하는지 동적으로 결정한다.\n",
        "* soft attention, absolute position attention, multi-head attention 등이 있음\n",
        "\n",
        "### 1-5. 가변 길이 표현\n",
        "* 가변 길이 입력에 대해 고정적인 벡터를 만드는 과정에서 생기는 문제가 있음(고정 길이 표현의 정보 병목 문제)\n",
        "* 길이의 차이가 많아도 고정적인 벡터로 표현을 할 수 있어야함. 그러기엔 아무래도 한계가 있음.\n",
        "* 어텐션 메커니즘 등을 사용하면 고정적인 벡터여도 가변 길이 입력을 잘 표현할 수 있음.\n",
        "\n",
        "### 1-6. 텍스트 압축률\n",
        "* 입력 데이터를 토큰화를 잘 하던지 문장의 필요없는 부분을 제외한다던지 해서 크기를 줄이는 것?\n",
        "* 토큰화, 서브워드 유닛 적용 이후의 표현이 원본 텍스트에 비해 더 간결한 정도를 의미함."
      ],
      "metadata": {
        "id": "hmCnWAajXyP7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<중략...>"
      ],
      "metadata": {
        "id": "GVDoXpcFhP02"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3-1. Related Work\n",
        "* SMT 즉, 통계 기반 기계 번역은 모르는 단어에 대한 처리가 힘들다\n",
        "\n",
        "* SMT에선 '이름(고유명사)'같이 어원이 불명확한 단어가 모르는 단어의 대부분이라고 한다.\n",
        "> ex) Tesla를 독일어로 번역해도 Tesla라고 표기함.\n",
        "\n",
        "* 위의 경우는 알파벳을 공유하기 때문에 그대로 복사해도 된다. 하지만 한글의 경우 '테슬라'로 번역될 것이다.\n",
        "\n",
        "* 문자 기반 번역은 구문 기반 모델(PBM)과 함께 연구되었고, 밀접한 관련 언어(한국어-일본어)에 대해 효과적이다.\n",
        "> 즉, 문자 기반 번역이란 애초에 효과적인 방법이다?\n",
        "\n",
        "* 합성어를 쪼개는 방법(알고리즘이 다양함)도 SMT에서 사용되어왔다. -> NMT에서 더 효과적인?\n",
        "\n",
        "* PBMT는 분할 규칙이 보수적임 -> 좀 더 진보적인 분할 규칙을 사용하는 것이 목표임. 물론 해냄\n",
        "> 그 규칙은 곧.. BPE, WPT등에 사용되는 서브워딩 기법을 보면 이해됨. (OOV해결을 위한)\n",
        "\n",
        "* 서브워딩은 작업마다 다름, 음절로 나누기, 음성으로 나누기 등등..\n",
        "\n",
        "* 다양한 나라의 언어를 쪼갤때는 일반화된 알고리즘을 사용하도록 제안됨. 하나의 나라에 국한되면 좋진 않으니까?\n",
        "\n",
        "* 단어 기반 접근 방식과 BPE, WPT 방식은 다른 것이다.\n",
        "단어 기반 접근 방식과 문자 기반 접근 방식은 다르다.\n",
        "\n",
        "* 어텐션 메커니즘이 어떤 수준에서 작동하는지(단어, 문자, 구)\n",
        "\n",
        "* 서브워드 단위 방식은 고정 길이 표현의 정보 병목을 막을 수 있다.\n",
        "\n",
        "* NMT는 PBMT와 다르게 시간 및 공간 효율성을 높이고 백오프 모델이 필요없도록 어휘 크기를 최소화함.\n",
        "\n",
        "* 텍스트 자체를 컴팩트하게 표현하는 것도 원한다.\n",
        "\n",
        "* BPE방식을 사용하여 압축률이 높은 어휘를 학습할 수 있다.\n"
      ],
      "metadata": {
        "id": "Vg0jGKQBbKG-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3-2. Byte Pair Encoding\n",
        "\n",
        "* 데이터 압축 기술중 하나임.\n",
        "\n",
        "* 바이트 쌍을 병합함.\n",
        "\n",
        "* 심볼 어휘로 문자를 사용함(바이트 단위)\n",
        "\n",
        "* 각 단어를 심볼 어휘 즉, 바이트 시퀀스로 나타냄. 단어 끝엔 '.'을 추가하여 단어의 끝임을 알림\n",
        "\n",
        "* 반복적으로 모든 쌍을 세고 가장 빈도가 높은 쌍을 하나의 심볼로 대체함.\n",
        "\n",
        "* 별도의 사전 목록이 필요가 없다. 빈번한 문자나 n-gram이 결국 하나의 심볼이 되기 때문이다.\n",
        "\n",
        "* 최종 심볼 어휘의 수 = (초기 심볼 수) + (병합 횟수)\n",
        "> 초기 심볼은 바이트 단위\n",
        "\n",
        "* 각 단어는 독립적으로 토큰화가 진행됨. 입력 텍스트에서 추출한 단어 딕셔너리에서 동작함. (value는 단어의 빈도수)\n",
        "\n",
        "아래는 코드 예시임\n"
      ],
      "metadata": {
        "id": "mD-_tNH5WKlS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re, collections\n",
        "def get_stats(vocab):\n",
        "    pairs = collections.defaultdict(int)\n",
        "    for word, freq in vocab.items():\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols)-1):\n",
        "            pairs[symbols[i],symbols[i+1]] += freq\n",
        "    return pairs\n",
        "\n",
        "\n",
        "def merge_vocab(pair, v_in):\n",
        "    v_out = {}\n",
        "    bigram = re.escape(' '.join(pair))\n",
        "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
        "    for word in v_in:\n",
        "        w_out = p.sub(''.join(pair), word)\n",
        "        v_out[w_out] = v_in[word]\n",
        "    return v_out\n",
        "\n",
        "vocab = {'l o w </w>' : 5, 'l o w e r </w>' : 2, 'n e w e s t </w>':6,'w i d e s t </w>':3}\n",
        "num_merges = 10\n",
        "\n",
        "for i in range(num_merges):\n",
        "    pairs = get_stats(vocab)\n",
        "    best = max(pairs, key=pairs.get)\n",
        "    # key=paris.get은 딕셔너리의 value를 기준으로 삼겠다는 의미\n",
        "    # 빈도수가 같으면 딕셔너리중 먼저 나온 값이 반환됨.\n",
        "    # 이 순서로 결과가 바뀔 수도 있기는 할 것이여!\n",
        "    vocab = merge_vocab(best, vocab)\n",
        "    print(f'pairs: {pairs}')\n",
        "    print(f'bset: {best}\\n')\n",
        "\n",
        "print(f'최종 vocab: {vocab}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GYFVOU9p0X4",
        "outputId": "e40dba5e-5868-4adc-90b1-16b174735e79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pairs: defaultdict(<class 'int'>, {('l', 'o'): 7, ('o', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 8, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('e', 's'): 9, ('s', 't'): 9, ('t', '</w>'): 9, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'e'): 3})\n",
            "bset: ('e', 's')\n",
            "\n",
            "pairs: defaultdict(<class 'int'>, {('l', 'o'): 7, ('o', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'es'): 6, ('es', 't'): 9, ('t', '</w>'): 9, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'es'): 3})\n",
            "bset: ('es', 't')\n",
            "\n",
            "pairs: defaultdict(<class 'int'>, {('l', 'o'): 7, ('o', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'est'): 6, ('est', '</w>'): 9, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est'): 3})\n",
            "bset: ('est', '</w>')\n",
            "\n",
            "pairs: defaultdict(<class 'int'>, {('l', 'o'): 7, ('o', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'est</w>'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3})\n",
            "bset: ('l', 'o')\n",
            "\n",
            "pairs: defaultdict(<class 'int'>, {('lo', 'w'): 7, ('w', '</w>'): 5, ('w', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'est</w>'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3})\n",
            "bset: ('lo', 'w')\n",
            "\n",
            "pairs: defaultdict(<class 'int'>, {('low', '</w>'): 5, ('low', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('w', 'est</w>'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3})\n",
            "bset: ('n', 'e')\n",
            "\n",
            "pairs: defaultdict(<class 'int'>, {('low', '</w>'): 5, ('low', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('ne', 'w'): 6, ('w', 'est</w>'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3})\n",
            "bset: ('ne', 'w')\n",
            "\n",
            "pairs: defaultdict(<class 'int'>, {('low', '</w>'): 5, ('low', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('new', 'est</w>'): 6, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3})\n",
            "bset: ('new', 'est</w>')\n",
            "\n",
            "pairs: defaultdict(<class 'int'>, {('low', '</w>'): 5, ('low', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3})\n",
            "bset: ('low', '</w>')\n",
            "\n",
            "pairs: defaultdict(<class 'int'>, {('low', 'e'): 2, ('e', 'r'): 2, ('r', '</w>'): 2, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'est</w>'): 3})\n",
            "bset: ('w', 'i')\n",
            "\n",
            "최종 vocab: {'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'wi d est</w>': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 이 논문에서는 BPE를 사용하며, 모든 쌍을 인덱싱하고 데이터 구조를 점진적으로 업데이트함으로써 효율을 높였다고 한다.\n",
        "* 즉, 이 코드 자체가 BPE는 아님, BPE 기법을 사용하여 단어를 문자단위부터 점차적으로 합쳐나가며 토큰화에 사용될 하나의 심볼들로 만든 것임.\n"
      ],
      "metadata": {
        "id": "f8bx7b8kp1t9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 가변 길이 인코딩을 수행하기 위한 다른 압축 알고리즘과의 차이점이 있다.\n",
        "\n",
        "* 심볼 시퀀스가 계속하여 서브워드 단위로 해석이 가능하고, 이러한 기법을 기반으로 훈련 시에 보지 못한 새로운 단어를 번역할 수 있게 한다.\n",
        "\n",
        "* 분절된 바이트단위 문자는 더 큰, 존재할법한? 심볼로 병합이 된다.\n",
        "\n",
        "* 예시로 {‘low’, ‘lowest’, ‘newer’, ‘wider’} 을 학습하고 'lower'를 입력했다 하자. 위 코드의 결과인 심볼 시퀀스에 'low', 'er'이 있기 때문에 해당 입력은 'low+er'로 분절화가 된다.\n",
        "\n"
      ],
      "metadata": {
        "id": "rrXDJ2ZF7B_Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 이 논문에선 소스 데이터를 위한 BPE, 타겟 데이터를 위한 BPE가 독립적인 경우와 joint BPE의 경우를 모두 다룬다.\n",
        "\n",
        "* Learning two independent encodings (source and target vocabulary):\n",
        "\n",
        "    * 소스 언어와 타겟 언어 각각에 대해 독립적인 BPE 인코딩을 학습합니다.\n",
        "    * 각 언어에 대해 훈련된 텍스트를 기반으로 각각의 언어에 특화된 서브워드를 생성합니다.\n",
        "    * 두 언어 간에 서로 다른 토크나이징이 가능하며, 텍스트와 어휘 크기 측면에서 각각의 언어에 특화된 효율성을 제공합니다.\n",
        "\n",
        "* Learning the encoding on the union of the two vocabularies (joint BPE):\n",
        "\n",
        "    * 소스 언어와 타겟 언어의 어휘를 합쳐 하나의 BPE 인코딩을 학습합니다.\n",
        "    * 두 언어 간에 서로 다른 토크나이징을 사용하지 않고, 하나의 통일된 서브워드 토크나이징을 적용합니다.\n",
        "    * 언어 간에 토크나이징 일관성을 유지하며, 텍스트 및 어휘 크기 측면에서 효율적입니다.\n",
        "\n",
        "* BPE를 소스, 타겟에 독립적으로 적용하면 같은 이름이 다르게 분절될 수 있음. 이는 매핑 문제로 이어짐.\n",
        "\n",
        "* 두 언어간 텍스트 일관성을 높이기 위해 사용하는 방법이 있음\n",
        "    \n",
        "    * 러시아어 -> ISO-9 사용(국제 표준) -> 라틴 문자\n",
        "    * BPE 적용\n",
        "    * 결과를 키릴 문자로 표기하면 러시아어 원래 표기로 돌아감\n"
      ],
      "metadata": {
        "id": "5SV_jxU8-5eZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Evaluation(평가)\n"
      ],
      "metadata": {
        "id": "6dBLVZzQ5YgB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> 알아두면 좋은 추가정보\n",
        "\n",
        "* WMT(Workshop on Machine Translation): 2015의 공유 번역 과제는 기계 번역에 관한 워크샵에서 제공한 데이터를 기반으로 한 번역 실험을 의미합니다. WMT는 기계 번역 연구 및 개발을 위한 국제 워크샵으로, 매년 번역 시스템을 평가하기 위한 공동 번역 과제를 제공합니다. 2015년 WMT의 공유된 번역 과제는 그 해에 제공된 데이터를 사용하여 번역 모델을 평가하고 비교하는 데 사용되었습니다.\n",
        "\n",
        "* truecasing: 텍스트에서 대문자와 소문자를 올바르게 구분하여 적용하는 작업.\n",
        "\n",
        "* [moses](https://www2.statmt.org/moses/): Moses는 기계 번역을 위한 오픈 소스 툴킷 중 하나로, 대표적인 구현 중 하나입니다. Moses는 통계적 기계 번역 (Statistical Machine Translation, SMT) 시스템을 구축하고 사용하기 위한 다양한 도구와 라이브러리를 제공합니다. 주로 명령 줄 인터페이스를 통해 상호 작용하며, 다양한 작업에 사용됩니다.\n",
        "\n",
        "* BELU(Bilingual Evaluation Understudy): 기계 번역 결과를 평가하는 데 사용되는 자동 평가 지표 중 하나. BLEU는 먼저 인간 평가자들이 번역을 평가한 결과를 참조(reference)로 사용하고, 이를 기반으로 자동적으로 번역 결과의 품질을 측정합니다.\n",
        "\n",
        "* CHRF3: CHRF3는 BLEU와 마찬가지로 기계 번역 결과를 평가하는 데 사용되는 자동 평가 지표 중 하나입니다. Popovic ́에 의해 제안된 이 지표는 번역 결과의 품질을 측정하는 데에 있어 BLEU보다 더 효과적인 결과를 내는 경우가 있습니다.\n",
        "\n",
        "* CHRF3는 문자 n-그램 F3 점수로, 특히 영어를 기반으로 한 번역에서 인간 판단과 잘 관련되어 있다고 발견되었습니다.\n",
        "\n",
        "* unigram F1: 모델이 생성한 텍스트와 실제 정답 텍스트 간의 유사성을 측정하는 데에 사용됨. 정밀도와 재현율의 조화평균을 사용하여 성능을 평가함.\n",
        "\n",
        "* word alignment\n",
        "\n",
        "```\n",
        "[\"나는\", \"너를\", \"사랑해\"]\n",
        "[\"I\", \"love\", \"you\"]\n",
        "```\n",
        "각 언어가 서로 대응되는 정보를 담고 있는 것.\n",
        "\n",
        "대표적으로 Pharaoh format이 있음(fast align도 이 방식임)\n",
        "\n",
        "```\n",
        "한국어 to 영어\n",
        "\n",
        "0-0 1-2 2-1  \n",
        "```\n",
        "\n",
        "```\n",
        "영어 to 한국어\n",
        "\n",
        "0-0 2-1 1-2\n",
        "```"
      ],
      "metadata": {
        "id": "7qJvMl1vCYYO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> NMT에서 이전에 보지 못한 단어들의 번역을 서브워드 단위로 표현함으로써 개선할 수 있을까?  \n",
        "\n",
        "> 단어 크기, 텍스트 크기 및 번역 품질 측면에서 어떤 서브워드 단위의 분절이 가장 우수한 성능을 보일까?"
      ],
      "metadata": {
        "id": "SlRFKFXmBM_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* WMT의 공유 과제에 대한 내용이 나옴\n",
        "\n",
        "* 영어 -> 독일어, train_set: 약 4.2백만개 문장 쌍 or 약 1억개의 토큰\n",
        "\n",
        "* 영어 -> 러시아어, train_set: 약 2.6백만개 문장 쌍 or 약 5천만개의 토큰\n",
        "\n",
        "* 사용한 개발 셋\n",
        "    * newstest2013를 개발용으로 사용\n",
        "    * newstest2014, 2015에 제출하여 점수를 얻었다?\n",
        "\n",
        "* 사용한 평가 지표\n",
        "    * BELU, CHRF3\n",
        "\n",
        "* 모르는 단어의 번역과 관련이 있기 때문에 이에 대한 통계를 보여줄 것임. 그 통계는 유니그램 F1을 통해 측정함.\n",
        "\n",
        "* 모든 실험은 Groundghog를 사용했음. https://github.com/sebastien-j/LV_groundhog/blob/master/experiments/nmt/README.md\n",
        "\n"
      ],
      "metadata": {
        "id": "QZJc-c9zCDB6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 사용된 세팅\n",
        "\n",
        "* hidden layer 1000\n",
        "\n",
        "* embedding layer 620\n",
        "\n",
        "* shortlist 30000\n",
        "\n"
      ],
      "metadata": {
        "id": "ncEJN39mLy2U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 학습 세팅\n",
        "\n",
        "* Adadelta\n",
        "    * minibatch=80, reshuffle between epochs\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WiN5yvzgMJXy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 약 7일동안 4개의 최종 모델을 만들고 고정된 임베딩 층으로 12시간동안 학습을 시킴.\n",
        "\n",
        "* 각 모델(최종 4개)에 대해 cut-off gradient clipping을 한 번은 5.0, 한 번은 1.0으로 설정. 1.0일 때가 더 좋았음.\n",
        "\n",
        "* newstest2013에서 가장 우수한 성능을 보인 시스템을 보고함\n",
        "\n",
        "* 총 8개의 모델에 대해서도 보고함\n",
        "\n",
        "* 문장 길이에 따라 정규화된 확률을 사용, 빔 탐색크기는 12.\n",
        "\n",
        "* fast-align을 사용함. 이는 희귀한 단어의 백오프 사전으로 사용됨.\n",
        "\n",
        "* 실험에서 번역 속도를 높이기 위해 딕셔너리를 사용함.\n",
        "\n",
        "* 후보 번역 목록에 대한 softmax를 수행할 때 필터링된 목록만 사용함."
      ],
      "metadata": {
        "id": "Pu3HO2PKMc4x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4-1. Subword statistics\n",
        "\n",
        "* 실험을 통해 확인할 번역 품질 외에도 주요 목표는 콤팩트하고 고정 크기의 서브워드 어휘를 통해 open-vocabulary를 표현하고 효율적인 훈력 및 디코딩을 가능하게 하는 것이다.\n",
        "\n",
        "* 표1을 참고\n",
        "\n",
        "* n-gram에서 어떤 n값을 선택하느냐에 따라 어휘 크기와 시퀀스 길이 간의 균형을 조절할 수 있습니다.\n",
        "\n",
        "* 시퀀스 길이의 증가가 상당하다면 가장 빈번한 k개의 단어 유형을 분할하지 않은 상태로 두는 방법이 있다.\n",
        "\n",
        "* 유니그램 표현만이 Open-vocabulary이다. 하지만 예비 실험에서 성능이 좋지 않았다. 그래서 바이그램 표현으로 했다. 그러나 테스트 세트의 일부 토근은 훈련 세트로 생성할 수 없었다.\n",
        "\n",
        "* 이전 SMT 연구에서 유용하다고 알려진 여러 단어 분절 기술이 있다.\n",
        "    * 빈도 기반 복합 분리(Koehn and Knight, 2003)\n",
        "    * 규칙 기반 음절 나눔(Liang, 1983)\n",
        "    * Morfessor(Creutz and Lagus, 2002)\n",
        "\n",
        "* 이 기법들이 어휘 크기를 줄이긴 하지만 알려지지 않은 단어 문제는 해결하지 못했다. 백오프 사전 없으면 부적합하다.\n",
        "\n",
        "* BPE는 open-vocabulary 목표를 충족하고 알려지지 않은 단어 문제도 해결해줌.\n",
        "\n",
        "* BPE 최고, Table 1 보셈 딱 봐도 좋잖슴\n",
        "\n",
        "* 실제로는 드물게 나타나는 서브워드 단위는 제외했음."
      ],
      "metadata": {
        "id": "KOjkjFyxR_gc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4-2. Translation experiments\n",
        "\n",
        "* WDict: 백오프 사전을 사용하는 단어 수준 모델\n",
        "* WUnk: 백오프 사전을 사용하지 않고, 없는 단어 UNK\n",
        "> 백오프 사전은 unigramF1을 향상시킨다. 하지만 이름을 표기하는 데 한정적이기 때문에 영어 -> 러시아어의 경우 향상이 작을 수 있다.\n",
        "* 모든 서브워드 시스템은 백오프 사전 없이 작동한다.\n",
        "\n",
        "* 모든 시스템이 기준 모델 대비 unigram F1 개선이 있었다. 특히 희귀한 단어의 경우\n",
        "\n",
        "```\n",
        "EN→DE의 경우 36.8% → 41.8%; EN→RU의 경우 26.5% → 29.7%\n",
        "\n",
        "```\n",
        "\n",
        "* OOV는 알파벳이 같은 경우(영어->독일어) 복사 전략은 잘 동작했다. 그러나 알파벳이 다른 경우(영어->러시아어)엔 서브워드 모델이 훨씬 결과가 좋았다.\n",
        "\n",
        "* 합친 BPE(BPE-J90k)가 가장 좋았음. 모든 서브워드 분할은 백오프 보다 우수하다.\n",
        "\n",
        "* BLEU와 CHRF3 지표의 불일치는 이 지표들 각각이 정밀도나 리콜과 같은 다른 편향을 가지고 있어서 발생\n",
        "\n",
        "* 암튼 성능 지표를 분석했음, NMT 성능 변동성은 여전히 열려 있는 문제라고 함."
      ],
      "metadata": {
        "id": "jzDcwKWxZOge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Analysis\n"
      ],
      "metadata": {
        "id": "UlUoWuYQjkl5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5-1. Unigram accuracy\n",
        "\n",
        "* 우리의 주요 주장은 단어 수준 NMT 모델에서 드문 및 알려지지 않은 단어의 번역이 부족하며, 서브워드 모델이 이러한 단어 유형의 번역을 개선한다는 것이다.\n",
        "\n",
        "* 빈도가 낮은 단어에 대한 Unigram F1은 항상 감소했다. 그런데 baseline시스템의 경우 OOV에 대한 F1이 좋았다. 그 이유는 '이름'을 복사하는 것이 효과적이기 때문이다.\n",
        "\n",
        "* 그냥 소스에서 복사된 이름의 경우를 제외하면 서브워드 시스템이 좋다.\n",
        "\n"
      ],
      "metadata": {
        "id": "181z4vEDjoVO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\"가장 빈도가 높은 50,000 단어에 대해서는 모든 신경망에서 표현이 동일하며, 이 범주에서 모든 신경망이 유사한 유니그램 F1을 달성합니다. 빈도 순위 50,000부터 500,000까지의 간격에서 C2-3/500k와 C2-50k 간의 비교에서 흥미로운 차이가 나타납니다. 두 시스템은 단일 유닛으로 이 구간의 단어를 나타내는 shortlist 크기만 다르며, C2-3/500k는 단일 유닛으로 나타내고, C2-50k는 서브워드 유닛을 통해 나타냅니다. C2-3/500k의 성능은 빈도 순위 500,000까지 심각하게 저하되며, 여기서 모델이 서브워드 표현으로 전환되고 성능이 회복됩니다. C2-50k의 성능은 더 안정적인 편입니다. 이는 서브워드 유닛이 단어보다 덜 희소하다는 사실 때문입니다. 우리의 훈련 세트에서 빈도 순위 50,000은 훈련 데이터에서 60의 빈도에 해당하며, 빈도 순위 500,000은 2의 빈도에 해당합니다. 서브워드 표현은 덜 희소하므로 신경망 어휘 크기를 줄이고 더 많은 단어를 서브워드 유닛을 통해 나타내면 성능이 향상될 수 있습니다.\""
      ],
      "metadata": {
        "id": "H2PL60HZjroE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"F1 숫자는 시스템 간의 일부 정성적인 차이를 숨기고 있습니다. 영어→독일어의 경우, WDict는 몇 개의 OOV(26.5% recall)를 생성하지만 높은 정밀도(60.6%)를 갖습니다. 반면, 서브워드 시스템은 높은 리콜을 달성하지만 낮은 정밀도를 갖습니다. 문자 바이그램 모델 C2-50k는 가장 많은 OOV 단어를 생성하며, 이 범주에 대해 상대적으로 낮은 정밀도(29.1%)를 달성합니다. 그러나 리콜(33.0%)에서는 백오프 사전을 능가합니다. BPE-60k는 분할 불일치로 인한 전사(또는 복사) 오류로 고통받아 정밀도(32.4%)가 약간 향상되었지만 리콜(26.6%)이 악화되었습니다. BPE-J90k의 경우 BPE-60k와 달리 분할 불일치로 인한 전사 오류를 개선하여 정밀도(38.6%)와 리콜(29.8%)이 모두 향상되었습니다.\""
      ],
      "metadata": {
        "id": "PWH-Se0Tlsx8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. 결론\n",
        "\n",
        "OOV에 대해서 BPE 토크나이저 사용이 효과가 있었다."
      ],
      "metadata": {
        "id": "tNQk5xoniozM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tfFXUFhYi9t2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}